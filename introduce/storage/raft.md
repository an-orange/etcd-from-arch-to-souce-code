## Raft协议

### 摘要
Raft是一个管理副本日志的共识算法。 它的作用跟Paxos一样，并且比Paxos更加高效，但是它的架构跟Paxos有很大的不同。这使得raft比Paxos
更容易理解，并能更有效的应用在生产系统中。为了让算法更容易理解，raft将关键组件拆分成了不同部分，比如Leader 选举，日志副本，安全，和状态机。
结果使得用户可以更加容易的学习使用raft算法。Raft同时提供了一种新的机制来改变集群成员，以此来保证安全性。

### 简介
共识算法使得一群节点可以作为一个节点组来一起合作对外提供服务，并且在内部允许部分节点的失效。 正因如此，他们在高可用大规模系统中扮演着重要的角色。
在过去的数十年间，Paxos一直占据着绝对的主导地位。绝大多数的共识算法的实现都是基于Paxos或者受到Paxos算法的影响。此外在共识算法的学习中，Paxos
也是一门必修课。

不幸的是，Paxos算法非常难理解，即使后续做了很多简化理解的尝试，但还是未能改变此种现状。 此外，Paxos复杂的架构设计在实现时也困难重重。这也导致不管是
系统工程师还是共识算法理论的学习者都深受其困。

在跟Paxos算法缠斗许久之后，我们终于发现了一种新的共识算法，它不仅可以让系统的构建更加容易，而且也能让学习者更容易理解。我们采取了非比寻常的方式，
只为让共识算法变更容易理解并且更利于在系统架构中使用。所以我们问自己： 我们可以定义一种共识算法，它比Paxos更实用，更易于理解。同时，我们也希望此算法
能够成为系统构建的核心算法，变成系统架构中最重要的基础设施。 所以不仅仅算法能够正常工作变的至关重要，对算法的观察跟监控也相当重要。

当然，我们做到了，它就是Raft共识算法。 在设计Raft算法的时候我们使用了很多特殊的技术手段来提高它的可理解性，包括分解（raft被分解成了Leader选举，日志副本，安全）
以及状态的减少。（相比于Paxos， raft减少了不确定性的程度以及服务器之间不一致的方式）通过对43个学生理解情况的对比，我们得出了Raft算法比Paxos算法更容易理解的结论:
在学习两种算法之后，43个人中有33人能够理解Raft算法，而能够理解Paxos算法的人却屈指可数。

Raft在很多方面跟现存的共识算法都比较类似, 但是它也具有一些新的特性：
- 强壮的Leader
raft 使用了比其他共识算法更健壮的领导关系。 比如： 日志只能从leader到其他的服务器。 这种策略简化了日志副本的管理，并且让raft更加容易理解。
- Leader 选举
raft使用了随机时间选举leader。任何的共识算法都会检测节点的心跳，raft算法在此基础上添加了一点小小的机制，也就是随机时间，来解决冲突。
- 成员变化
raft管理集群中成员变化的机制是通过一种称为联合共识的方式进行的。 

我们相信raft是要比Paxos和其他共识算法更优的算法，无论是用于教育还是工程实现。它要更为简单和易于实现。 对于生产系统来说它的描述足够完整。
raft有几个开源的实现，并且已经被一些公司所采用。它的安全性已经经过了实践证明，同时效率和性能也要优于一般的共识算法。

简介剩余的第二部分介绍了复制状态机，第三部分讨论了Paxos算法的优劣，第四部分描述可理解的一般性方法，第5-8部分介绍raft算法，
第9部分对raft算法进行评估， 最后第10部分描述相关工作。

### 复制状态机
共识算法通常出现在复制状态机的上下文中。在这种方法中, 服务器集合上的状态机模型会保证集群具有相同的状态跟副本，即使有一些服务宕机也可以继续运作。
复制状态机主要用于分布式状态中解决分区容错的问题。比如: 具有单Leader节点的大规模集群，如GFS，HDFS以及RAMCloud，通常使用复制状态机来管理leader选举和存储配置信息
，并且即使在leader奔溃之后也能保持。比如在Zookeeper和Chubby中都使用了复制状态机。

复制状态机经典的实现方式是采用日志副本， 正如下图所示，每一个服务存储一个包含一系列命令的日志副本，状态机按次序以此执行。每一份日志在同样的次序下保存了相同的命令
，所以每一个状态机处理相同的命令序列。 因为状态机是确定性的， 所以每一个计算机具有相同的状态和相同的输出序列。 

保持副本日志的一致性是共识算法的工作。服务器上的共识模块从客户端接收命令并将其添加到他们的日志当中。 它与其他服务器上的共识算法进行交互，
确保每一份日志最终都会保证在相同的序列下保持相同的命令，即使一些服务失效了。 一旦命令被复制，每一个服务的复制状态机将在日志序列中处理他们，并且输出会返回给客户端。
结果，服务组成一个单一格式的，高可用的状态机。

经典的共识算法系统都具有以下特性：
- 安全：
（绝对不会返回不正确的结果）在所有的非拜占庭容错条件下， 包括网络依赖，分区，丢包，复制或者重新排序等情况下都能确保安全。
- 高可用
只要服务中主要的服务节点可以操作并且彼此通信，那么它就是完整可用的。 因此一个典型的5个节点的集群允许2个节点失效。服务恢复之后又可以重新加入集群。
- 保持日志一致性的时候不依赖时钟
错误的始终和极端的消息延迟，在最坏的情况下，也只是会影响可用性，并不影响一致性。
- 命令的完成取决于集群中的绝大多数节点：
一般来说，只要集群中绝大多数的节点作出了响应，则认为此次命令完成。个别表现较慢的服务器， 不会影响到整个系统的性能。

### Paxos 算法
在过去的数十年间，Paxos算法几乎编程了共识算法的代名词。 它是最普遍被用来教学的协议。绝大多数的共识算法的实施都是以此为基础。
Paxos首次定义了一个可以就单一节点达成共识的协议。 比如，单节点日志副本。我们将此子集称为单一法令Paxos算法。Paxos将多个实例组合在一起来处理
一系列的决策，如日志(多-Paxos) Paxos确保安全和活跃度, 并且支持集群成员改变，它的正确性已经得到了证明，在使用中也很高效。

不幸的是，Paxos有两个大的缺点。 第一个缺点是Paxos很难理解。 对Paxos算法完整的解释非常不透明。鲜有人能够理解，需要花费很大的经历。
所以在下面的参考目录中有一些文章做响应的解释，但是这些解释也仅仅面向的是单一指令的Paxos，即使如此，也已经很难理解了。

### 为可理解而设计
在设计raft算法的时候有几个目标： 它必须是一个完整的并且实用的分布式系统基础设施。 所以它减少了很多需要开发者做的设计工作； 无论在任何条件下，它都必须
保证安全性和可用性；对普通使用者来说，还需要保证高效。 但这些并不是我们最重要的目标，我们最重要的目标是可理解性。它必须被绝大多数开发者所理解, 这样系。
者就可以在实际的系统中使用他们。 在设计raft算法的时候，有几个点需要关注，我们必须权衡并作出选择。 在我们评估可理解性的时候，我们会问： 解释起来有多难?
实现起来有多难？

我们意识到，这种分析具有高度的主观性： 尽管如此，我们还是使用了两种一般适用性的技术。第一个就是众所周知的问题分解。我们将问题拆解成了几个可以被解决，被解释，
被独立理解的部分。比如说，我们将raft协议拆分为了leader选举，日志复制，安全和成员管理几部分。

第二个就是做减法，我们通过减少状态的数量来减少状态空间， 使得系统更加连贯并消除一些可能的不确定性。尤其对于日志来说，不允许有遗漏， raft限制了日志彼此可能不一致的方式。
尽管在绝大多数情况下我们尽力去消除不确定性， 但在一些情况下，不确定性反而提高了可理解性。尤其是，随机数本身就具有不确定性，但是他们是非常好的方式去处理所有的选择问题。 

我们在raft算法中，通过随机数的方式来进行leader选举。


### Raft 共识算法
在第二部分的描述中，我们知道raft是一种管理副本日志的共识算法。 图二对共识算法做了一个概述。 图三列举了算法核心的特性。在后续的内容中，会逐个进行详细的讨论。
raft实现共识算法首先选举一个leader，然后赋予leader完整的能力去管理日志副本。 leader接受客户端的输入，然后将其复制到其他服务器，并且当其将日志复制到其他服务器
之后进行广播。通过Leader的方式可以简化副本日志的管理。 比如， leader可以在不经过跟其他节点共识的前提下决定将新的输入放入到日志的什么位置，并且数据的流向始终是从
leader到其他服务器。Leader也可以失效或者与其他服务器失连，这种情况下，将会选举一个新的Leader节点。

![](\_asserts\images\raft_2.jpg)
通过Leader的方式，raft算法将一个共识问题，拆分成了三个相关的子问题
- Leader 选举
当系统中没有leader节点时，必须选举产生新的Leader
- 日志副本
Leader必须从客户端接受日志的输入并将其复制到其他服务器。
- 安全
最关键的raft的安全特性是图三所示的状态机安全性。

![](\_asserts\images\saft_raft.jpg)

在展示了共识算法之后，下一部分讨论高可用以及不同时刻节点角色的转换。

#### 5.1 Raft基础
一个raft集群包含几个服务器。5个节点是最常见的数量，这样就可以容忍系统最多出现两个节点失效。

![](https://raw.githubusercontent.com/csunny/etcd-from-arch-to-souce-code/master/_asserts/images/state_machine.jpg)

#### 参考
- [raft](https://raft.github.io)